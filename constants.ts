export const GEMINI_MODEL_NAME = 'gemini-2.5-flash-preview-04-17';
export const GEMINI_IMAGE_BATCH_SIZE = 5; // Number of images to process in one API call

export const GEMINI_API_REQUEST_PROMPT_TEMPLATE = (userQuery: string): string => `
You are an expert image analysis AI. Your task is to identify objects in the provided image(s) based on a user's query and return their bounding boxes.

For each image provided in the input parts, analyze it and generate a corresponding JSON array of detected objects.
The final output MUST be a single JSON array, where each element of this top-level array is itself an array of bounding box objects for the corresponding input image, maintaining the original order of images.

Each detected object within an image's array must be a JSON object with the following structure:
{
  "label": "object_label_string",
  "x_min": 0.0,
  "y_min": 0.0,
  "width": 0.0,
  "height": 0.0
}
All coordinates (x_min, y_min, width, height) MUST be normalized, ranging from 0.0 to 1.0, relative to the image's dimensions. 'x_min' and 'y_min' are the top-left corner of the box.

If no objects matching the query are found in a specific image, return an empty array [] for that image's entry in the main array.

The user's query for object detection is: "${userQuery}"

Example for 2 input images:
If image 1 has two objects and image 2 has one object, the response should be:
[
  [
    { "label": "cat", "x_min": 0.1, "y_min": 0.1, "width": 0.2, "height": 0.2 },
    { "label": "table", "x_min": 0.5, "y_min": 0.4, "width": 0.3, "height": 0.3 }
  ],
  [
    { "label": "dog", "x_min": 0.3, "y_min": 0.2, "width": 0.15, "height": 0.25 }
  ]
]

Example for 1 input image with no objects found:
[
  []
]

Ensure your entire response strictly adheres to this JSON format. Do not include any explanatory text outside of the JSON structure.
`;

export const GEMINI_ASSIST_PROMPT_TEMPLATE = (
  userQuery: string,
  existingAnnotations?: { label: string; x_min: number; y_min: number; width: number; height: number }[]
): string => `
You are an AI assistant helping a user refine or add annotations to an image.
The user has provided an image and a query. They may have also provided existing annotations.

Your task is to respond to the user's query by providing new bounding boxes or suggestions.
The response MUST be a JSON array of bounding box objects. Each object must have:
{
  "label": "object_label_string",
  "x_min": 0.0,
  "y_min": 0.0,
  "width": 0.0,
  "height": 0.0
}
All coordinates (x_min, y_min, width, height) MUST be normalized (0.0-1.0).

User's query: "${userQuery}"

${
  existingAnnotations && existingAnnotations.length > 0
    ? `The user has already identified the following objects. Consider these to avoid significant overlaps unless the query implies refinement or finding related objects:
${JSON.stringify(existingAnnotations, null, 2)}`
    : 'There are no prior annotations provided for this specific query.'
}

Based on the user's query and any existing annotations, identify relevant objects and provide their bounding boxes.
If the query asks a question about an area or object, try to provide a label that answers it or annotates the subject of the question.
If no new objects matching the query are found, or if the query is a question that cannot be answered with a bounding box, return an empty array [].

Ensure your entire response is a single JSON array of bounding box objects. Do not include any explanatory text outside of this JSON structure.
`;


// Predefined distinct colors for bounding boxes
export const BOUNDING_BOX_COLORS: string[] = [
  '#FF6B6B', // Red
  '#4ECDC4', // Cyan
  '#45B7D1', // Blue
  '#FED766', // Yellow
  '#2AB7CA', // Darker Cyan
  '#F0B67F', // Orange
  '#A2D0C1', // Light Teal
  '#FFD166', // Light Orange
  '#06D6A0', // Green
  '#EF476F', // Pink
  '#118AB2', // Dark Blue
  '#073B4C', // Navy
];

export const YOLO_DATASET_YAML_TEMPLATE = (
    relativePathToImages: string, // e.g., ../images
    numberOfClasses: number,
    classNames: string[] // e.g., ['person', 'car']
  ): string => `
# YOLOv5/v8 dataset configuration YAML
# Generated by AI Image Annotator

# Dataset paths
# Adjust 'path' if your dataset.yaml is not in a 'data' folder directly adjacent to 'images' and 'labels'
path: ..  # Root directory from where 'train' and 'val' paths are relative.
train: ${relativePathToImages} # path to train images folder, relative to 'path'
val: ${relativePathToImages}   # path to val images folder, relative to 'path' (use same for simplicity or create a split)
# test:               # path to test images folder (optional)

# Classes
nc: ${numberOfClasses}
names: [${classNames.map(name => `'${name}'`).join(', ')}]

# Download script/URL (optional)
# download:

# Other configurations (optional)
# batch_size: 16
# workers: 8
`;


export const TRAIN_BAT_SCRIPT_TEMPLATE = `
@echo off
echo Starting YOLO Training (Windows Batch Script)
echo.
echo Make sure you have your Python environment with YOLO and PyTorch activated.
echo For example, if using Conda:
echo   conda activate your_yolo_env
echo.
echo Please edit this script to include the correct command for your YOLO framework.
echo.
echo Example for Ultralytics YOLOv8:
echo   yolo train data=dataset.yaml model=yolov8n.pt epochs=100 imgsz=640 batch=16 device=0
echo.
echo Example for Ultralytics YOLOv5:
echo   python train.py --img 640 --batch 16 --epochs 100 --data data/dataset.yaml --weights yolov5s.pt --name my_yolo_run
echo.
echo Make sure 'dataset.yaml' is correctly referenced.
echo If 'dataset.yaml' is in a 'data' subdirectory, the command might be:
echo   yolo train data=data/dataset.yaml ...
echo.
pause
REM Add your YOLO training command below:
REM yolo train ... OR python train.py ...
`;

export const TRAIN_PS1_SCRIPT_TEMPLATE = `
Write-Host "Starting YOLO Training (Windows PowerShell Script)"
Write-Host ""
Write-Host "Make sure you have your Python environment with YOLO and PyTorch activated."
Write-Host "For example, if using Conda:"
Write-Host "  conda activate your_yolo_env"
Write-Host ""
Write-Host "Please edit this script to include the correct command for your YOLO framework."
Write-Host ""
Write-Host "Example for Ultralytics YOLOv8:"
Write-Host "  yolo train data=dataset.yaml model=yolov8n.pt epochs=100 imgsz=640 batch=16 device=0"
Write-Host ""
Write-Host "Example for Ultralytics YOLOv5:"
Write-Host "  python train.py --img 640 --batch 16 --epochs 100 --data data/dataset.yaml --weights yolov5s.pt --name my_yolo_run"
Write-Host ""
Write-Host "Make sure 'dataset.yaml' is correctly referenced."
Write-Host "If 'dataset.yaml' is in a 'data' subdirectory, the command might be:"
Write-Host "  yolo train data=data/dataset.yaml ..."
Write-Host ""
Read-Host -Prompt "Press Enter to continue (after editing this script with your command)"

# Add your YOLO training command below:
# yolo train ... OR python train.py ...
`;

export const TRAIN_SH_SCRIPT_TEMPLATE = `
#!/bin/bash
echo "Starting YOLO Training (Linux/macOS Bash Script)"
echo ""
echo "Make sure you have your Python environment with YOLO and PyTorch activated."
echo "For example, if using Conda:"
echo "  conda activate your_yolo_env"
echo "Or if using a virtual environment:"
echo "  source path/to/your_venv/bin/activate"
echo ""
echo "Please edit this script to include the correct command for your YOLO framework."
echo ""
echo "Example for Ultralytics YOLOv8:"
echo "  yolo train data=dataset.yaml model=yolov8n.pt epochs=100 imgsz=640 batch=16 device=0"
echo ""
echo "Example for Ultralytics YOLOv5:"
echo "  python train.py --img 640 --batch 16 --epochs 100 --data data/dataset.yaml --weights yolov5s.pt --name my_yolo_run"
echo ""
echo "Make sure 'dataset.yaml' is correctly referenced."
echo "If 'dataset.yaml' is in a 'data' subdirectory, the command might be:"
echo "  yolo train data=data/dataset.yaml ..."
echo ""
read -p "Press Enter to continue (after editing this script with your command)"

# Add your YOLO training command below:
# yolo train ... OR python train.py ...
`;
